{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mt2286/.conda/envs/brainlm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from random import randint, seed\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "from transformers import ViTImageProcessor, ViTMAEConfig\n",
    "from brainlm_mae.modeling_vit_mae_with_padding import ViTMAEForPreTraining \n",
    "from brainlm_mae.replace_vitmae_attn_with_flash_attn import replace_vitmae_attn_with_flash_attn\n",
    "from toolkit.BrainLM_Toolkit import convert_fMRIvols_to_A424, convert_to_arrow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMRI Data Arrow Conversion Starting...\n",
      "There's no A24 Coordinates dat file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not processing 0 files due to insufficient fMRI data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.23it/s]\n",
      "Getting normalization stats: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Normalizing Data:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing Data: 100%|██████████| 1/1 [00:00<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(424, 645)\n",
      "Print data array:  (424, 645)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 55.68 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 424/424 [00:02<00:00, 188.53 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data_dir = \"/home/mt2286/project/BrainLM/toolkit/sample_dataset/raw_fMRI_data\"\n",
    "save_data_dir = \"/home/mt2286/project/BrainLM/toolkit/sample_dataset/a424_fMRI_data\" #Make sure this directory exists.\n",
    "args = {\n",
    "    \"uk_biobank_dir\": \"/home/mt2286/project/BrainLM/toolkit/sample_dataset/a424_fMRI_data\",     # \"Path to directory containing dat files, A424 coordinates file, and A424 excel sheet.\",\n",
    "    \"arrow_dataset_save_directory\": os.path.join(save_data_dir,\"arrow_form\"),     # \"The directory where you want to save the output arrow datasets.\"\n",
    "    \"dataset_name\": \"Test_data_arrow_norm\",\n",
    "}\n",
    "\n",
    "# convert_fMRIvols_to_A424(data_path=raw_data_dir, output_path=save_data_dir)\n",
    "convert_to_arrow_datasets(args, args[\"arrow_dataset_save_directory\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot Inference - Saving CLS Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_vitmae_attn_with_flash_attn()\n",
    "params = \"650M\" #Choose between 650M and 111M\n",
    "config = ViTMAEConfig.from_pretrained(\"vandijklab/brainlm\", subfolder=f\"vitmae_{params}\")\n",
    "config.update({\n",
    "    \"mask_ratio\": 0.75,\n",
    "    \"timepoint_patching_size\": 20,\n",
    "    \"num_timepoints_per_voxel\": 200,\n",
    "    \"hidden_dropout_prob\": 0.0,\n",
    "    \"attention_probs_dropout_prob\": 0.0,\n",
    "    \"output_attentions\": True,\n",
    "})\n",
    "\n",
    "model = ViTMAEForPreTraining.from_pretrained(\n",
    "        \"vandijklab/brainlm\",\n",
    "        config=config,\n",
    "        subfolder=f\"vitmae_{params}\",\n",
    "    ).to(device)\n",
    "\n",
    "model = model.half()\n",
    "model.eval()\n",
    "# print(model.dtype)\n",
    "# print(model.config.mask_ratio)\n",
    "# print(model.vit.embeddings.config.mask_ratio)\n",
    "\n",
    "do_r2 = True\n",
    "do_inference = True\n",
    "aggregation_mode = \"cls\" # 'cls', 'mean', or 'max'\n",
    "\n",
    "variable_of_interest_col_name = \"Index\"\n",
    "image_column_name = \"All_Patient_All_Voxel_Normalized_Recording\"\n",
    "length = 200\n",
    "num_voxels = 424\n",
    "\n",
    "# need this if running on matteo's branch, due to multiple train modes (auto-encoder, causal attention, predict last, etc)\n",
    "try:\n",
    "    print(model.config.train_mode)\n",
    "except AttributeError:\n",
    "    model.config.train_mode = \"auto_encode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_ds = load_from_disk(\"/home/mt2286/project/BrainLM/toolkit/sample_dataset/a424_fMRI_data/Arrow_Datasets2/Brain_Region_Coordinates\")\n",
    "train_ds = load_from_disk(\"/home/mt2286/project/BrainLM/toolkit/sample_dataset/a424_fMRI_data/Arrow_Datasets2/train/\")\n",
    "# val_ds = load_from_disk(\"/gpfs/gibbs/pi/dijk/BrainLM_Datasets/UKB_Large_rsfMRI_and_tffMRI_Arrow_WithRegression_v3_with_metadata/val_ukbiobank\")\n",
    "# test_ds = load_from_disk(\"/gpfs/gibbs/pi/dijk/BrainLM_Datasets/UKB_Large_rsfMRI_and_tffMRI_Arrow_WithRegression_v3_with_metadata/test_ukbiobank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = f\"/gpfs/gibbs/pi/dijk/BrainLM_zero_inf/{model_name}/dataset_{dataset_v}/\"\n",
    "if not os.path.exists(dir_name) and do_inference:\n",
    "    os.makedirs(dir_name)\n",
    "dataset_split = {\"train\": train_ds, \"val\": val_ds, \"test\": test_ds, \"concat\": concat_ds}\n",
    "ds_used = dataset_split[split]\n",
    "print(ds_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = ViTImageProcessor(size={\"height\": model.config.image_size[0], \"width\": model.config.image_size[1]})\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    size = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "voxel_x_coords_list = coords_ds[\"Y\"]\n",
    "reorder_idxs_by_x_coord = sorted(range(len(voxel_x_coords_list)), key=lambda k: voxel_x_coords_list[k])\n",
    "reorder_idxs_by_x_coord = np.array(reorder_idxs_by_x_coord)\n",
    "max_val_to_scale = 5.6430855"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(examples):\n",
    "    \"\"\"Preprocess a batch of images by applying transforms.\"\"\"\n",
    "    fmri_images_list = []\n",
    "    for idx in range(len(examples[image_column_name])):\n",
    "        signal_window = torch.tensor(examples[image_column_name][idx], dtype=torch.float32).t()\n",
    "\n",
    "        # Choose random starting index, take window of moving_window_len points for each region\n",
    "        start_idx = randint(0, signal_window.shape[0] - length)\n",
    "        end_idx = start_idx + length\n",
    "        signal_window = signal_window[start_idx: end_idx, :]\n",
    "        signal_window = torch.movedim(signal_window, 0, 1)  # --> [num_voxels, moving_window_len]\n",
    "\n",
    "        # reorder voxels according to x-coordinate\n",
    "        signal_window = signal_window[reorder_idxs_by_x_coord, :]\n",
    "        signal_window = signal_window / max_val_to_scale\n",
    "\n",
    "        # Repeat tensor for 3 channels (R,G,B)\n",
    "        signal_window = signal_window.unsqueeze(0).repeat(3, 1, 1)\n",
    "\n",
    "        fmri_images_list.append(signal_window) \n",
    "\n",
    "\n",
    "    examples[\"pixel_values\"] = fmri_images_list  # No transformation or resizing; model will do padding\n",
    "    return examples\n",
    "\n",
    "\n",
    "def get_attention_cls_token(attn_probs):\n",
    "    attn_probs_heads = attn_probs[31].squeeze(0) \n",
    "    attn_probs_avg = attn_probs_heads.mean(dim=0, keepdim=True)\n",
    "    cls_attn = attn_probs_avg[:, 0, :].cpu().numpy()\n",
    "    return cls_attn\n",
    "embarc_ds.set_transform(preprocess_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type=\"pad\"\n",
    "list_cls_tokens = []\n",
    "list_attn_cls_tokens = []\n",
    "all_embeddings = []\n",
    "all_index = []\n",
    "with torch.no_grad():\n",
    "    for recording in tqdm(embarc_ds, desc=\"Getting CLS tokens\"):\n",
    "\n",
    "        pixel_values = recording[\"pixel_values\"].unsqueeze(0).half().to(device)\n",
    "        if model_type == \"pad\":\n",
    "            # pixel_values is [batch, channels=3, 424, 200]. Pad to [batch, channels=3, 432, 432]\n",
    "            height_pad_total = model.config.image_size[0] - pixel_values.shape[2]\n",
    "            height_pad_total_half = height_pad_total // 2\n",
    "            width_pad_total = model.config.image_size[1] - pixel_values.shape[3]\n",
    "            width_pad_total_half = width_pad_total // 2\n",
    "            pixel_values = F.pad(pixel_values, (width_pad_total_half, width_pad_total_half, height_pad_total_half, height_pad_total_half), \"constant\", -1)\n",
    "\n",
    "        encoder_output = model.vit(\n",
    "            pixel_values=pixel_values,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        cls_token = encoder_output.last_hidden_state[:,0,:]  # torch.Size([1, 256])\n",
    "        embedding = encoder_output.last_hidden_state[:,1:,:]\n",
    "        all_embeddings.append(embedding.detach().cpu().numpy())\n",
    "        list_cls_tokens.append(cls_token.detach().cpu().numpy())\n",
    "        # all_index.append(recording[\"labels\"].detach().numpy())\n",
    "        attn_cls_token = get_attention_cls_token(encoder_output.attentions)\n",
    "        list_attn_cls_tokens.append(attn_cls_token)\n",
    "print(all_embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_name = os.path.join(sav_dir, f'{params}_cls_token.npy')\n",
    "print(\"Saving inference results to: \", preds_name)\n",
    "np.save(preds_name, list_attn_cls_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
