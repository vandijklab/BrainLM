{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aace52ac-c202-4d70-86cb-bffb49995299",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BrainLM Tutorial Notebook\n",
    "BrainLM is a foundation model for brain activity recordings. The GitHub repo can be found here: https://github.com/vandijklab/BrainLM\n",
    "\n",
    "**Using this tutorial notebook, you will learn how to apply BrainLM to your own fMRI data!**\n",
    "\n",
    "**Capabilities include:**\n",
    "> - **Easy-to-use Preprocessing functions** to format your fMRI dataset for BrainLM input\n",
    "> - **Finetuned BrainLM models** for clinical metadata prediction (Age, Anxiety, PTSD, Neuroticism)\n",
    "> - **Pretrained BrainLM models** for Zero-shot (clinical) Regression and downstream finetuning\n",
    "> - \"Putting your dataset in perspective\" via joint-embedding with 40k+ UKBiobank subjects\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e3f9b6-b64b-4290-b271-b6a46af18084",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Before You Begin \n",
    "- Your fMRI data must be minimally preprocessed using standard fMRI preprocessing procedures. Here are the preprocessing [scripts](https://www.fmrib.ox.ac.uk/ukbiobank/fbp/) and [documentation](https://biobank.ctsu.ox.ac.uk/crystal/crystal/docs/brain_mri.pdf) for the UKBiobank Imaging Pipeline.\n",
    "- **Conda Environment:** Make sure you're using the correct conda environment. \n",
    ">- For now: `/vast/palmer/home.mccleary/sr2464/.conda/envs/cell_lm_flash_attn`\n",
    ">-  Eventually: The default BrainLM conda environment on GitHub\n",
    ">-  Before camera ready: we need to merge these. Eg nilearn and other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44bc599b-670a-4de8-b010-542a34f4e277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mt2286/.conda/envs/brainlm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'brainlm_mae.vit_image_finetuning_mlp_pred_head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# from brainlm_mae.modeling_brainlm import BrainLMForPretraining\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# from utils.brainlm_trainer import BrainLMTrainer\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m# from brainlm_mae.configuration_brainlm import BrainLMConfig\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m# from brainlm_mae.brainlm_finetuning_mlp_pred_head import BrainLMForFinetuning\u001b[39;00m\n\u001b[1;32m     26\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m../\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbrainlm_mae\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvit_image_finetuning_mlp_pred_head\u001b[39;00m \u001b[39mimport\u001b[39;00m ViTMAEForFinetuning\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbrainlm_mae\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvit_image_finetune_config\u001b[39;00m \u001b[39mimport\u001b[39;00m ViTMAEFinetuneConfig\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbrainlm_mae\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_vit_mae_with_padding\u001b[39;00m \u001b[39mimport\u001b[39;00m ViTMAEForPreTraining \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'brainlm_mae.vit_image_finetuning_mlp_pred_head'"
     ]
    }
   ],
   "source": [
    "#Importing Libraries\n",
    "import os\n",
    "import math\n",
    "from random import randint, seed\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "from transformers import ViTImageProcessor, ViTMAEConfig\n",
    "# from brainlm_mae.modeling_brainlm import BrainLMForPretraining\n",
    "# from utils.brainlm_trainer import BrainLMTrainer\n",
    "# from brainlm_mae.configuration_brainlm import BrainLMConfig\n",
    "# from brainlm_mae.brainlm_finetuning_mlp_pred_head import BrainLMForFinetuning\n",
    "sys.path.append(\"../\")\n",
    "from brainlm_mae.vit_image_finetuning_mlp_pred_head import ViTMAEForFinetuning\n",
    "from brainlm_mae.vit_image_finetune_config import ViTMAEFinetuneConfig\n",
    "from brainlm_mae.modeling_vit_mae_with_padding import ViTMAEForPreTraining \n",
    "from brainlm_mae.replace_vitmae_attn_with_flash_attn import replace_vitmae_attn_with_flash_attn\n",
    "from utils.brainlm_trainer_log_wandb_only import BrainLMTrainer\n",
    "from BrainLM_Toolkit import convert_fMRIvols_to_A424, convert_fMRI_dat_files_to_arrow_dataset\n",
    "from trial import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd771e4-d43f-4565-a1e6-ab09a98edd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762baf5f-c610-48c0-9c83-a246d3a59896",
   "metadata": {},
   "source": [
    "# 1. Convert data to Arrow Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec12dd6-5735-4061-9909-bf4fd38ada02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UKBioBank FMRI Data Arrow Conversion Starting...\n",
      "There's no A24 Coordinates dat file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting normalization stats: 100%|██████████| 1513/1513 [00:34<00:00, 43.92it/s]\n",
      "Normalizing Data:   0%|          | 0/1513 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print data array:  (645, 424)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (645,424) (424,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39muk_biobank_dir\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m/home/jo548/palmer_scratch/EMERGE/matrices\u001b[39m\u001b[39m\"\u001b[39m,     \u001b[39m# \"Path to directory containing dat files, A424 coordinates file, and A424 excel sheet.\",\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39marrow_dataset_save_directory\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m/home/mt2286/palmer_scratch/EMERGE_prep\u001b[39m\u001b[39m\"\u001b[39m,     \u001b[39m# \"The directory where you want to save the output arrow datasets.\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdataset_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mEMERGE\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mnormalization_info_file\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mhastily_generated_normalization_data_1000\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m }\n\u001b[0;32m----> 7\u001b[0m convert_to_arrow_datasets(args, args[\u001b[39m\"\u001b[39;49m\u001b[39marrow_dataset_save_directory\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m/gpfs/gibbs/project/dijk/mt2286/BrainLM-dev/BrainLM_Toolkit/trial.py:166\u001b[0m, in \u001b[0;36mconvert_to_arrow_datasets\u001b[0;34m(args, save_path)\u001b[0m\n\u001b[1;32m    160\u001b[0m     recording_mean_subtracted2[:, voxel_idx] \u001b[39m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m         recording_mean_subtracted2[:, voxel_idx] \u001b[39m-\u001b[39m voxel_mean\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[39m#Voxelwise Robust Scaler Normalization\u001b[39;00m\n\u001b[1;32m    165\u001b[0m recording_mean_subtracted3 \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 166\u001b[0m     recording_mean_subtracted3 \u001b[39m-\u001b[39;49m\n\u001b[1;32m    167\u001b[0m     normalization_info_dataset_v3[\u001b[39m'\u001b[39;49m\u001b[39mdata_median_per_voxel\u001b[39;49m\u001b[39m'\u001b[39;49m][:, np\u001b[39m.\u001b[39;49mnewaxis]\n\u001b[1;32m    168\u001b[0m ) \u001b[39m/\u001b[39m normalization_info_dataset_v3[\u001b[39m'\u001b[39m\u001b[39mIQR\u001b[39m\u001b[39m'\u001b[39m][:, np\u001b[39m.\u001b[39mnewaxis]\n\u001b[1;32m    170\u001b[0m \u001b[39mfor\u001b[39;00m idx_voxel \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(recording_mean_subtracted3\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[1;32m    171\u001b[0m     recording_mean_subtracted3[:, idx_voxel] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(\n\u001b[1;32m    172\u001b[0m             recording_mean_subtracted3[:, idx_voxel],\n\u001b[1;32m    173\u001b[0m             normalization_info_dataset_v3[\u001b[39m'\u001b[39m\u001b[39m_1th_percentile_per_voxel_robustScaling\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    174\u001b[0m             normalization_info_dataset_v3[\u001b[39m'\u001b[39m\u001b[39m_99th_percentile_per_voxel_robustScaling\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    175\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (645,424) (424,1) "
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"uk_biobank_dir\": \"/home/jo548/palmer_scratch/EMERGE/matrices\",     # \"Path to directory containing dat files, A424 coordinates file, and A424 excel sheet.\",\n",
    "    \"arrow_dataset_save_directory\": \"/home/mt2286/palmer_scratch/test\",     # \"The directory where you want to save the output arrow datasets.\"\n",
    "    \"dataset_name\": \"EMERGE\",\n",
    "}\n",
    "convert_to_arrow_datasets(args, args[\"arrow_dataset_save_directory\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d310838-8765-41f7-b7bc-f81667a9aa39",
   "metadata": {},
   "source": [
    "# 3. Clinical Variable Prediction using Finetuned BrainLM models\n",
    "\n",
    "**Current Capabilities:** Age, PTSD (PCL-5), Anxiety (GAD-7), Neuroticism\n",
    "\n",
    "- ask Syed/Antonio for which notebooks are most up. todate.\n",
    "- my best guess is inference_01_finetuned_model_ViT_image.ipynb on branch antonio_dev_syed_working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7bb0e93-3e04-44cf-a49a-a2909ad2ae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model\n",
    "modality = \"Age.At.MHQ\" # Choose from [\"Age.At.MHQ\", \"PHQ9.Severity\", \"PCL.Score\", \"GAD7.Severity\", \"Neuroticism\", \"Depressed.At.Baseline\", \"Self.Harm.Ever\", \"Not.Worth.Living\", \"Gender\"]\n",
    "ft_model = f\"/gpfs/gibbs/pi/dijk/BrainLM_mihir_files/BrainLM_fine_{modality}_mihir_run/111M/\"\n",
    "ft_modelname=\"finetune_age_FromScratch_Nov24th_1225_111M\"\n",
    "# config = ViTMAEFinetuneConfig.from_pretrained(ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e16ce471-4a30-4c2d-87a7-fe00ea173472",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ViTMAEFinetuneConfig.from_pretrained(\"vandijklab/brainlm_finetuned\", subfolder=\"age\")\n",
    "model = ViTMAEForFinetuning.from_pretrained(\"vandijklab/brainlm_finetuned\", config=config, subfolder=\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4aff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset\n",
    "# coords_ds = load_from_disk(\"/home/mt2286/project/BrainLM-dev/BrainLM_Toolkit/output/Arrow_Datasets2/Test_data_arrow_norm/Brain_Region_Coordinates\")\n",
    "# train_ds = load_from_disk(\"/home/mt2286/project/BrainLM-dev/BrainLM_Toolkit/output/Arrow_Datasets2/Test_data_arrow_norm/train\")\n",
    "# hcp_ds = load_from_disk(\"/gpfs/gibbs/pi/dijk/HCP_Arrow_WithRegression/test_hcp\")\n",
    "coords_ds = load_from_disk(\"/home/mt2286/palmer_scratch/EMBARC_prep/Brain_Region_Coordinates\")\n",
    "# train_ds = load_from_disk(\"/gpfs/gibbs/pi/dijk/BrainLM_Datasets/UKB_Large_rsfMRI_and_tffMRI_Arrow_WithRegression_v3_with_metadata/val_ukbiobank\")\n",
    "# val_ds = load_from_disk(\"/gpfs/gibbs/pi/dijk/BrainLM_Datasets/UKB_Large_rsfMRI_and_tffMRI_Arrow_WithRegression_v3_with_metadata/val_ukbiobank\")\n",
    "train_ds = load_from_disk(\"/home/mt2286/palmer_scratch/EMBARC_prep/train_ukbiobank1000\")\n",
    "dataset_v = \"embarc\"\n",
    "split = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b276496",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_i = {\"rs\": 0, \"tf\": 1}\n",
    "task_type = pd.Series(train_ds['Filename']).str.split(\"_\", expand=True)[:][1].map(task_to_i)\n",
    "train_ds = train_ds.add_column(name=\"Task_Type\", column=task_type)\n",
    "\n",
    "# task_type = pd.Series(val_ds['Filename']).str.split(\"_\", expand=True)[:][1].map(task_to_i)\n",
    "# val_ds = val_ds.add_column(name=\"Task_Type\", column=task_type)\n",
    "\n",
    "# task_type = pd.Series(test_ds['Filename']).str.split(\"_\", expand=True)[:][1].map(task_to_i)\n",
    "# test_ds = test_ds.add_column(name=\"Task_Type\", column=task_type)\n",
    "concat_ds = train_ds\n",
    "# concat_ds = concatenate_datasets([train_ds, val_ds, test_ds])\n",
    "index = pd.Series(np.arange(concat_ds.num_rows))\n",
    "concat_ds = concat_ds.add_column(name=\"Index\", column=index)\n",
    "# index = pd.Series(np.arange(test_ds.num_rows))\n",
    "# test_ds = test_ds.add_column(name=\"Index\", column=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa16c266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving directory\n",
    "sav_dir = \"/gpfs/gibbs/pi/dijk/embarc_inf\"\n",
    "if not os.path.exists(sav_dir):\n",
    "    os.makedirs(sav_dir)\n",
    "\n",
    "# dataset_split = {\"train\": train_ds, \"val\": val_ds, \"test\": test_ds, \"concat\": concat_ds}\n",
    "dataset_split = {\"train\": concat_ds}\n",
    "ds_used = dataset_split[split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "17751af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After selecting non-nan metadata, have 3728 samples to finetune on.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices:   0%|          | 0/3728 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 3728/3728 [04:23<00:00, 14.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#No need to run this for inference.\n",
    "\n",
    "variable_of_interest_col_name = modality\n",
    "# recording_col_name = \"Voxelwise_RobustScaler_Normalized_Recording\"\n",
    "length = 200\n",
    "\n",
    "# Processing for metadata variable\n",
    "full_label_list = ds_used[variable_of_interest_col_name]\n",
    "non_nan_indices = [idx for idx in range(len(full_label_list)) if not math.isnan(full_label_list[idx])]\n",
    "\n",
    "non_nan_ds = ds_used.select(non_nan_indices)  # select samples which have non-nan values for metadata variable\n",
    "non_nan_ds = non_nan_ds.shuffle(seed=42)  # shuffle reproducibly to remove any ordering samples may have had\n",
    "print(f\"After selecting non-nan metadata, have {non_nan_ds.num_rows} samples to finetune on.\\n\\n\")\n",
    "labels_nonnan = non_nan_ds[variable_of_interest_col_name]  # get labels\n",
    "\n",
    "#Normalize variables: log(variable + 1) / max_val\n",
    "if variable_of_interest_col_name in [\"PCL.Score\", \"GAD7.Severity\"]:\n",
    "    labels_nonnan_log1p = np.log(np.array(labels_nonnan, dtype=np.float32) + 1.0)  # log base_e(value + 1)\n",
    "    labels_nonnan_log1p_divmax = np.divide(labels_nonnan_log1p, labels_nonnan_log1p.max())  # bring into range [0, 1]\n",
    "    labels_normalized = labels_nonnan_log1p_divmax.tolist()\n",
    "    pos_weight = None\n",
    "elif variable_of_interest_col_name == \"Age.At.MHQ\":\n",
    "    z_score_transform = StandardScaler()\n",
    "    labels_normalized_np = z_score_transform.fit_transform(np.expand_dims(np.array(labels_nonnan), axis=1))\n",
    "    labels_normalized_np = np.squeeze(labels_normalized_np, axis=1)\n",
    "    labels_normalized = labels_normalized_np.tolist()\n",
    "    np.save('mean.npy', z_score_transform.mean_)\n",
    "    np.save('std.npy', z_score_transform.scale_)\n",
    "    pos_weight = None\n",
    "elif variable_of_interest_col_name == \"Neuroticism\":\n",
    "    labels_nonnan_log1p = np.array(labels_nonnan, dtype=np.float32)  # if is Neuroticism, then no log transform, distribution is already good\n",
    "    labels_nonnan_log1p_divmax = np.divide(labels_nonnan_log1p, labels_nonnan_log1p.max())  # bring into range [0, 1]\n",
    "    labels_normalized = labels_nonnan_log1p_divmax.tolist()\n",
    "    pos_weight = None\n",
    "elif variable_of_interest_col_name == \"PHQ9.Severity\":\n",
    "    labels_normalized = [1 if num > 4.0 else 0 for num in labels_nonnan]\n",
    "    sum_ones = sum(labels_normalized)\n",
    "    sum_zeros = len(labels_normalized) - sum_ones\n",
    "    # pos_weight = sum_zeros / sum_ones\n",
    "    pos_weight = None\n",
    "elif variable_of_interest_col_name in [\"Depressed.At.Baseline\", \"Self.Harm.Ever\", \"Not.Worth.Living\"]:\n",
    "    labels_normalized = labels_nonnan\n",
    "    sum_ones = sum(labels_normalized)\n",
    "    sum_zeros = len(labels_normalized) - sum_ones\n",
    "    # pos_weight = sum_zeros / sum_ones\n",
    "    pos_weight = None\n",
    "elif variable_of_interest_col_name == \"Gender\":\n",
    "    labels_normalized = labels_nonnan\n",
    "    pos_weight = pos_weight = torch.tensor([2027 / 1687], dtype=torch.float32)\n",
    "else:\n",
    "    raise NotImplementedError(\"Unknown variable of interest specified.\")\n",
    "\n",
    "# Replace labels in concat_ds with normalized labels\n",
    "non_nan_ds = non_nan_ds.remove_columns(variable_of_interest_col_name)\n",
    "non_nan_ds = non_nan_ds.add_column(name=variable_of_interest_col_name, column=labels_nonnan)\n",
    "ds_used = non_nan_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1db0fe62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Raw_Recording', 'Voxelwise_RobustScaler_Normalized_Recording', 'All_Patient_All_Voxel_Normalized_Recording', 'Per_Patient_All_Voxel_Normalized_Recording', 'Per_Patient_Per_Voxel_Normalized_Recording', 'Per_Voxel_All_Patient_Normalized_Recording', 'Subtract_Mean_Normalized_Recording', 'Subtract_Mean_Divide_Global_STD_Normalized_Recording', 'Subtract_Mean_Divide_Global_99thPercent_Normalized_Recording', 'Filename', 'Patient ID', 'Task_Type', 'Index'],\n",
       "    num_rows: 1865\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26f4b7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "424\n",
      "350\n"
     ]
    }
   ],
   "source": [
    "subset = ds_used.select(range(10))\n",
    "\n",
    "# Now, extract the 'All_Patient_All_Voxel_Normalized_Recording' column from this subset\n",
    "first_five_instances = subset['Voxelwise_RobustScaler_Normalized_Recording']\n",
    "print(len(first_five_instances))\n",
    "print(len(first_five_instances[0]))\n",
    "print(len(first_five_instances[1][1]))\n",
    "\n",
    "# labels = subset[\"Age.At.MHQ\"]\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f433beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "424\n",
      "350\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "subset = embarc_ds.select(range(5))\n",
    "\n",
    "# Now, extract the 'All_Patient_All_Voxel_Normalized_Recording' column from this subset\n",
    "first_five_instances = subset['All_Patient_All_Voxel_Normalized_Recording']\n",
    "print(len(first_five_instances))\n",
    "print(len(first_five_instances[0]))\n",
    "print(len(first_five_instances[1][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "094ee21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering indices: 100%|██████████| 1865/1865 [00:00<00:00, 988547.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Raw_Recording', 'Voxelwise_RobustScaler_Normalized_Recording', 'All_Patient_All_Voxel_Normalized_Recording', 'Per_Patient_All_Voxel_Normalized_Recording', 'Per_Patient_Per_Voxel_Normalized_Recording', 'Per_Voxel_All_Patient_Normalized_Recording', 'Subtract_Mean_Normalized_Recording', 'Subtract_Mean_Divide_Global_STD_Normalized_Recording', 'Subtract_Mean_Divide_Global_99thPercent_Normalized_Recording', 'Filename', 'Patient ID', 'Task_Type', 'Index'],\n",
      "    num_rows: 1819\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fmri_recs = ds_used['Voxelwise_RobustScaler_Normalized_Recording']\n",
    "data_indices = [idx for idx in tqdm(range(len(ds_used)), desc=\"Filtering indices\") if len(fmri_recs[idx][0]) >= 200]\n",
    "embarc_ds = ds_used.select(data_indices)  \n",
    "print(embarc_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c791d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing fMRI images\n",
    "voxel_x_coords_list = coords_ds[\"Y\"]\n",
    "reorder_idxs_by_x_coord = sorted(range(len(voxel_x_coords_list)), key=lambda k: voxel_x_coords_list[k])\n",
    "reorder_idxs_by_x_coord = np.array(reorder_idxs_by_x_coord)\n",
    "max_val_to_scale = 5.6430855\n",
    "num_timepoints_per_voxel = 200 #model.config.num_timepoints_per_voxel\n",
    "image_column_name = \"Voxelwise_RobustScaler_Normalized_Recording\"\n",
    "# image_column_name = \"Voxelwise_RobustScaler_Normalized_Recording\"\n",
    "\n",
    "def preprocess_images(examples):\n",
    "    \"\"\"Preprocess a batch of images by applying transforms.\"\"\"\n",
    "    fmri_images_list = []\n",
    "    # label_list = []\n",
    "    # print('examples.keys(): ',examples.keys())\n",
    "    # print('len(examples[image_column_name][0]): ',len(examples[image_column_name][0]))\n",
    "    # print('examples[variable_of_interest_col_name][0]: ',examples[variable_of_interest_col_name][0])\n",
    "    for idx in range(len(examples[image_column_name])):\n",
    "        signal_window = torch.tensor(examples[image_column_name][idx], dtype=torch.float32).t()\n",
    "        # label = examples[variable_of_interest_col_name][0] # Original \n",
    "        # label = examples[variable_of_interest_col_name][idx]\n",
    "        # label = torch.tensor(label, dtype=torch.float32)\n",
    "        # print('[in preprocess_images] label: ',label)\n",
    "        # print(aaa)\n",
    "\n",
    "        # Choose random starting index, take window of moving_window_len points for each region\n",
    "        start_idx = randint(0, signal_window.shape[0] - num_timepoints_per_voxel)\n",
    "        end_idx = start_idx + num_timepoints_per_voxel\n",
    "        signal_window = signal_window[start_idx: end_idx, :]\n",
    "        signal_window = torch.movedim(signal_window, 0, 1)  # --> [num_voxels, moving_window_len]\n",
    "\n",
    "        # reorder voxels according to x-coordinate\n",
    "        signal_window = signal_window[reorder_idxs_by_x_coord, :]\n",
    "        signal_window = signal_window / max_val_to_scale\n",
    "\n",
    "        # Repeat tensor for 3 channels (R,G,B)\n",
    "        signal_window = signal_window.unsqueeze(0).repeat(3, 1, 1)\n",
    "\n",
    "        fmri_images_list.append(signal_window)\n",
    "        # label_list.append(label)\n",
    "    \n",
    "    # examples[\"pixel_values\"] = [transforms(image) for image in fmri_images_list]\n",
    "    examples[\"pixel_values\"] = fmri_images_list  # No transformation or resizing; model will do padding\n",
    "    # examples[\"label\"] = label_list\n",
    "    return examples\n",
    "\n",
    "# def collate_fn(examples):\n",
    "#     pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "#     labels = torch.stack([example[\"label\"] for example in examples])\n",
    "#     # print('labels: ',labels)\n",
    "#     # print(aaa)\n",
    "#     # labels = torch.tensor([1 for _ in range(len(pixel_values))])\n",
    "    \n",
    "#     return {\n",
    "#         \"pixel_values\": pixel_values,\n",
    "#         \"input_ids\": pixel_values,\n",
    "#         \"labels\": labels\n",
    "#     }\n",
    "embarc_ds.set_transform(preprocess_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa47c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only if modality is Age\n",
    "def reverse_z_score(normalized_data, mean_path='mean.npy', std_path='std.npy'):\n",
    "    # Load saved mean and standard deviation\n",
    "    mean = np.load(\"/home/mt2286/project/BrainLM-dev/BrainLM_Toolkit/mean.npy\")\n",
    "    std = np.load(\"/home/mt2286/project/BrainLM-dev/BrainLM_Toolkit/std.npy\")\n",
    "\n",
    "    if isinstance(normalized_data, list):\n",
    "        normalized_data_np = np.array(normalized_data)\n",
    "    else:\n",
    "        normalized_data_np = normalized_data\n",
    "\n",
    "    # Reshape if the input is a 1D array\n",
    "    if normalized_data_np.ndim == 1:\n",
    "        normalized_data_np = normalized_data_np.reshape(-1, 1)\n",
    "    \n",
    "    # Manually reverse the z-score normalization\n",
    "    reversed_np = (normalized_data_np * std) + mean\n",
    "\n",
    "    # If the output is expected to be 1D, squeeze the array\n",
    "    if reversed_np.shape[1] == 1:\n",
    "        reversed_np = np.squeeze(reversed_np, axis=1)\n",
    "    \n",
    "    reversed_data = reversed_np.tolist()\n",
    "    \n",
    "    return reversed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5f39cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:   0%|          | 0/1819 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:   1%|          | 10/1819 [00:06<20:10,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "if model.config.patch_size == 16:\n",
    "    resize_target_size = 432\n",
    "elif model.config.patch_size == 14:\n",
    "    resize_target_size = 434\n",
    "else:\n",
    "    raise RuntimeError(\"New patch size encountered, check init() of VitMAEForPreTraining\")\n",
    "\n",
    "res_arr = []\n",
    "i =0\n",
    "model = model.to(device)\n",
    "with torch.no_grad():  # No gradient tracking for inference\n",
    "    for batch in tqdm(embarc_ds, desc=\"Running inference\"):\n",
    "        pixel_values = batch[\"pixel_values\"].unsqueeze(0)\n",
    "        batch_size, channels, height, width = pixel_values.shape\n",
    "        \n",
    "        height_pad_total = resize_target_size - height\n",
    "        height_pad_total_half = height_pad_total // 2\n",
    "\n",
    "        width_pad_total = resize_target_size - width\n",
    "        width_pad_total_half = width_pad_total // 2\n",
    "\n",
    "        # Padding for the whole batch\n",
    "        pixel_values_padded = F.pad(pixel_values, (width_pad_total_half, width_pad_total_half, height_pad_total_half, height_pad_total_half), \"constant\", -1)\n",
    "        \n",
    "        encoder_output = model(\n",
    "            pixel_values_padded.to(device),\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=True,\n",
    "            is_train=False\n",
    "        )\n",
    "        \n",
    "        reversed_data = [reverse_z_score(output.cpu())[0] for output in encoder_output]\n",
    "        res_arr.extend(reversed_data)\n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d9beda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[58.777088008524586,\n",
       " 67.4934525273299,\n",
       " 55.103782586868576,\n",
       " 67.39009272091751,\n",
       " 59.04440160839704,\n",
       " 70.8334976345175,\n",
       " 54.453155665135796,\n",
       " 61.39124787648072,\n",
       " 72.17388075858094,\n",
       " 66.18419450638217]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_arr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b271b5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70.69828823933477,\n",
       " 70.59515816939594,\n",
       " 70.58661955696951,\n",
       " 70.59520931993517,\n",
       " 70.56480032436855,\n",
       " 70.64159227613803,\n",
       " 70.49952870869973,\n",
       " 70.66545047506102,\n",
       " 70.64024825162471,\n",
       " 70.67109467249226]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_arr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ff94945",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_name = os.path.join(sav_dir, f'{modality}_pred.npy')\n",
    "print(\"Saving inference results to: \", preds_name)\n",
    "np.save(preds_name, res_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20259234-5b04-4acb-80f1-2c347f6c176c",
   "metadata": {},
   "source": [
    "# 4. Zero-shot Clinical Regression using BrainLM mean-pool Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a47cb82-86dc-42e0-8a93-3ccc04c51631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "replace_vitmae_attn_with_flash_attn()  # Flash Attention\n",
    "seed(42)\n",
    "dataset_v = \"embarc\"\n",
    "split = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1262eaf9-2c71-4b46-ab7a-8bf457431d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and specify params\n",
    "params = \"650M\" #Choose between 650M and 111M\n",
    "model_name = f\"brainlm_vitmae_{params}_100pc_data_mr75\"\n",
    "model_path = f\"/gpfs/gibbs/pi/dijk/BrainLM_runs/huggingface_best_models/{model_name}\"\n",
    "config = ViTMAEConfig.from_pretrained(model_path)\n",
    "config.update({\n",
    "    \"mask_ratio\": 0.75,\n",
    "    \"timepoint_patching_size\": 20,\n",
    "    \"num_timepoints_per_voxel\": 200,\n",
    "    \"hidden_dropout_prob\": 0.0,\n",
    "    \"attention_probs_dropout_prob\": 0.0,\n",
    "    \"output_attentions\": True,\n",
    "})\n",
    "\n",
    "model = ViTMAEForPreTraining.from_pretrained(\n",
    "        model_path,\n",
    "        from_tf=bool(\".ckpt\" in model_path),\n",
    "        config=config,\n",
    "        # cache_dir=model_args.cache_dir,\n",
    "        # revision=model_args.model_revision,\n",
    "        # use_auth_token=True if model_args.use_auth_token else None,\n",
    "    ).to(device)\n",
    "\n",
    "model = model.half()\n",
    "model.eval()\n",
    "# print(model.dtype)\n",
    "# print(model.config.mask_ratio)\n",
    "# print(model.vit.embeddings.config.mask_ratio)\n",
    "\n",
    "do_r2 = True\n",
    "do_inference = True\n",
    "aggregation_mode = \"cls\" # 'cls', 'mean', or 'max'\n",
    "\n",
    "variable_of_interest_col_name = \"Index\"\n",
    "image_column_name = \"All_Patient_All_Voxel_Normalized_Recording\"\n",
    "length = 200\n",
    "num_voxels = 424\n",
    "\n",
    "# need this if running on matteo's branch, due to multiple train modes (auto-encoder, causal attention, predict last, etc)\n",
    "try:\n",
    "    print(model.config.train_mode)\n",
    "except AttributeError:\n",
    "    model.config.train_mode = \"auto_encode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff4bdd86-a527-46ff-841c-4b9c4d7272cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5446541.dat_rs\n",
      "5446541.dat_rs\n",
      "_rs\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "\n",
    "coords_ds = load_from_disk(\"/gpfs/gibbs/pi/dijk/BrainLM_Datasets/UKB_Large_rsfMRI_and_tffMRI_Arrow_WithRegression_v3_with_metadata/Brain_Region_Coordinates\")\n",
    "train_ds = load_from_disk(\"/gpfs/gibbs/pi/dijk/BrainLM_Datasets/UKB_Large_rsfMRI_and_tffMRI_Arrow_WithRegression_v3_with_metadata/train_ukbiobank\")\n",
    "\n",
    "val_ds = load_from_disk(\"/gpfs/gibbs/pi/dijk/BrainLM_Datasets/UKB_Large_rsfMRI_and_tffMRI_Arrow_WithRegression_v3_with_metadata/val_ukbiobank\")\n",
    "\n",
    "test_ds = load_from_disk(\"/gpfs/gibbs/pi/dijk/BrainLM_Datasets/UKB_Large_rsfMRI_and_tffMRI_Arrow_WithRegression_v3_with_metadata/test_ukbiobank\")\n",
    "\n",
    "\n",
    "print(train_ds[0]['Filename'])\n",
    "\n",
    "# Make Resting state vs Task column\n",
    "task_to_i = {\"rs\": 0, \"tf\": 1}\n",
    "# code is a bit wordy but it is the fastest\n",
    "task_type = pd.Series(train_ds['Filename']).str.split(\"_\", expand=True)[:][1].map(task_to_i)\n",
    "train_ds = train_ds.add_column(name=\"Task_Type\", column=task_type)\n",
    "\n",
    "task_type = pd.Series(val_ds['Filename']).str.split(\"_\", expand=True)[:][1].map(task_to_i)\n",
    "val_ds = val_ds.add_column(name=\"Task_Type\", column=task_type)\n",
    "\n",
    "task_type = pd.Series(test_ds['Filename']).str.split(\"_\", expand=True)[:][1].map(task_to_i)\n",
    "test_ds = test_ds.add_column(name=\"Task_Type\", column=task_type)\n",
    "\n",
    "concat_ds = concatenate_datasets([train_ds, val_ds, test_ds])\n",
    "concat_ds\n",
    "\n",
    "# make order label\n",
    "index = pd.Series(np.arange(concat_ds.num_rows))\n",
    "concat_ds = concat_ds.add_column(name=\"Index\", column=index)\n",
    "\n",
    "index = pd.Series(np.arange(test_ds.num_rows))\n",
    "test_ds = test_ds.add_column(name=\"Index\", column=index)\n",
    "\n",
    "# index = pd.Series(np.arange(HCP_ds.num_rows))\n",
    "# HCP_ds = HCP_ds.add_column(name=\"Index\", column=index)\n",
    "\n",
    "example0 = concat_ds[0]\n",
    "print(example0['Filename'])\n",
    "print(example0['Patient ID'])\n",
    "print(example0['Task_Type'])\n",
    "print(example0['Index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "918fef58-85e0-4b80-871a-d076770853e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Raw_Recording', 'Voxelwise_RobustScaler_Normalized_Recording', 'Filename', 'Patient ID', 'Order', 'eid', 'Gender', 'Age.At.MHQ', 'PHQ9.Severity', 'Depressed.At.Baseline', 'Neuroticism', 'Self.Harm.Ever', 'Not.Worth.Living', 'PCL.Score', 'GAD7.Severity', 'Task_Type', 'Index'],\n",
      "    num_rows: 7628\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Saving directory\n",
    "dir_name = f\"/gpfs/gibbs/pi/dijk/BrainLM_zero_inf/{model_name}/dataset_{dataset_v}/\"\n",
    "if not os.path.exists(dir_name) and do_inference:\n",
    "    os.makedirs(dir_name)\n",
    "dataset_split = {\"train\": train_ds, \"val\": val_ds, \"test\": test_ds, \"concat\": concat_ds}\n",
    "ds_used = dataset_split[split]\n",
    "print(ds_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03b7830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward pass through model, passing whole fMRI recording\n",
    "image_processor = ViTImageProcessor(size={\"height\": model.config.image_size[0], \"width\": model.config.image_size[1]})\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    size = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "voxel_x_coords_list = coords_ds[\"Y\"]\n",
    "reorder_idxs_by_x_coord = sorted(range(len(voxel_x_coords_list)), key=lambda k: voxel_x_coords_list[k])\n",
    "reorder_idxs_by_x_coord = np.array(reorder_idxs_by_x_coord)\n",
    "max_val_to_scale = 5.6430855"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6692d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(examples):\n",
    "    \"\"\"Preprocess a batch of images by applying transforms.\"\"\"\n",
    "    fmri_images_list = []\n",
    "    for idx in range(len(examples[image_column_name])):\n",
    "        signal_window = torch.tensor(examples[image_column_name][idx], dtype=torch.float32).t()\n",
    "\n",
    "        # Choose random starting index, take window of moving_window_len points for each region\n",
    "        start_idx = randint(0, signal_window.shape[0] - length)\n",
    "        end_idx = start_idx + length\n",
    "        signal_window = signal_window[start_idx: end_idx, :]\n",
    "        signal_window = torch.movedim(signal_window, 0, 1)  # --> [num_voxels, moving_window_len]\n",
    "\n",
    "        # reorder voxels according to x-coordinate\n",
    "        signal_window = signal_window[reorder_idxs_by_x_coord, :]\n",
    "        signal_window = signal_window / max_val_to_scale\n",
    "\n",
    "        # Repeat tensor for 3 channels (R,G,B)\n",
    "        signal_window = signal_window.unsqueeze(0).repeat(3, 1, 1)\n",
    "\n",
    "        fmri_images_list.append(signal_window) \n",
    "\n",
    "\n",
    "    examples[\"pixel_values\"] = fmri_images_list  # No transformation or resizing; model will do padding\n",
    "    return examples\n",
    "\n",
    "# def collate_fn(examples):\n",
    "#     pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "#     labels = torch.tensor([1 for _ in range(len(pixel_values))])\n",
    "#     return {\n",
    "#         \"pixel_values\": pixel_values,\n",
    "#         \"input_ids\": pixel_values,\n",
    "#         \"labels\": labels\n",
    "#     }\n",
    "def get_attention_cls_token(attn_probs):\n",
    "    attn_probs_heads = attn_probs[31].squeeze(0) \n",
    "    attn_probs_avg = attn_probs_heads.mean(dim=0, keepdim=True)\n",
    "    cls_attn = attn_probs_avg[:, 0, :].cpu().numpy()\n",
    "    return cls_attn\n",
    "embarc_ds.set_transform(preprocess_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ead6c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTMAEForPreTraining(\n",
       "  (vit): ViTMAEModel(\n",
       "    (embeddings): ViTMAEEmbeddings(\n",
       "      (patch_embeddings): ViTMAEPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "    )\n",
       "    (encoder): ViTMAEEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-31): 32 x ViTMAELayer(\n",
       "          (attention): ViTMAEAttention(\n",
       "            (attention): ViTMAESelfAttention(\n",
       "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTMAESelfOutput(\n",
       "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTMAEIntermediate(\n",
       "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTMAEOutput(\n",
       "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): ViTMAEDecoder(\n",
       "    (decoder_embed): Linear(in_features=1280, out_features=512, bias=True)\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-7): 8 x ViTMAELayer(\n",
       "        (attention): ViTMAEAttention(\n",
       "          (attention): ViTMAESelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTMAESelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTMAEIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTMAEOutput(\n",
       "          (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder_pred): Linear(in_features=512, out_features=588, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffe1f9b6-6e2e-450f-b1ff-e94924380e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting CLS tokens: 100%|██████████| 1819/1819 [12:06<00:00,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 240, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Save CLS Tokens + All Embeddings\n",
    "model_type=\"pad\"\n",
    "list_cls_tokens = []\n",
    "list_attn_cls_tokens = []\n",
    "all_embeddings = []\n",
    "all_index = []\n",
    "with torch.no_grad():\n",
    "    for recording in tqdm(embarc_ds, desc=\"Getting CLS tokens\"):\n",
    "\n",
    "        pixel_values = recording[\"pixel_values\"].unsqueeze(0).half().to(device)\n",
    "        if model_type == \"pad\":\n",
    "            # pixel_values is [batch, channels=3, 424, 200]. Pad to [batch, channels=3, 432, 432]\n",
    "            height_pad_total = model.config.image_size[0] - pixel_values.shape[2]\n",
    "            height_pad_total_half = height_pad_total // 2\n",
    "            width_pad_total = model.config.image_size[1] - pixel_values.shape[3]\n",
    "            width_pad_total_half = width_pad_total // 2\n",
    "            pixel_values = F.pad(pixel_values, (width_pad_total_half, width_pad_total_half, height_pad_total_half, height_pad_total_half), \"constant\", -1)\n",
    "\n",
    "        encoder_output = model.vit(\n",
    "            pixel_values=pixel_values,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        cls_token = encoder_output.last_hidden_state[:,0,:]  # torch.Size([1, 256])\n",
    "        embedding = encoder_output.last_hidden_state[:,1:,:]\n",
    "        all_embeddings.append(embedding.detach().cpu().numpy())\n",
    "        list_cls_tokens.append(cls_token.detach().cpu().numpy())\n",
    "        # all_index.append(recording[\"labels\"].detach().numpy())\n",
    "        attn_cls_token = get_attention_cls_token(encoder_output.attentions)\n",
    "        list_attn_cls_tokens.append(attn_cls_token)\n",
    "print(all_embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3ea2caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving inference results to:  /gpfs/gibbs/pi/dijk/embarc_inf/650M_cls_token.npy\n"
     ]
    }
   ],
   "source": [
    "preds_name = os.path.join(sav_dir, f'{params}_cls_token.npy')\n",
    "print(\"Saving inference results to: \", preds_name)\n",
    "np.save(preds_name, list_attn_cls_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a8c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if aggregation_mode == \"cls\":\n",
    "    print(\"cls aggregation\")\n",
    "    all_embeds = np.concatenate(list_cls_tokens, axis=0)\n",
    "elif aggregation_mode == \"mean\":\n",
    "    print(\"mean pool aggregation\")\n",
    "    all_mean_embeddings = [e.mean(axis=1) for e in all_embeddings]\n",
    "    all_embeds = np.concatenate(all_mean_embeddings, axis=0)\n",
    "elif aggregation_mode == \"max\":\n",
    "    print(\"max pool aggregation\")\n",
    "    all_sum_embeddings = [e.max(axis=1) for e in all_embeddings]\n",
    "    all_embeds = np.concatenate(all_sum_embeddings, axis=0)   \n",
    "\n",
    "\n",
    "print(all_embeds.shape)\n",
    "all_index = np.concatenate(all_index, axis=0)\n",
    "\n",
    "# check if dataloader messed up order in any way\n",
    "if not np.all(all_index[:-1] <= all_index[1:]):\n",
    "    # reorder everything\n",
    "    print(\"reordering\")\n",
    "    all_embeds = all_embeds[all_index, :]\n",
    "    \n",
    "np.save(f\"{dir_name}{split}_{aggregation_mode}_all_{length}recordinglength.npy\", all_embeds)\n",
    "\n",
    "\n",
    "# extract patch tokens with data\n",
    "if aggregation_mode != \"cls\":\n",
    "    print(length)\n",
    "    hor_img_start_idx = (model.config.image_size[1] - length) // 2\n",
    "    hor_token_start = hor_img_start_idx // model.config.patch_size\n",
    "    hor_token_end = hor_token_start + np.ceil(length / model.config.patch_size).astype(int)\n",
    "\n",
    "    vert_img_start_idx = (model.config.image_size[0] - num_voxels) // 2\n",
    "    vert_token_start = vert_img_start_idx // model.config.patch_size\n",
    "    vert_token_end = vert_token_start + np.ceil(num_voxels / model.config.patch_size).astype(int)\n",
    "    print(hor_token_start, hor_token_end, vert_token_start, vert_token_end)\n",
    "\n",
    "    for i, e in enumerate(all_embeddings):\n",
    "        e = e.reshape(e.shape[0], int(np.sqrt(e.shape[1])), int(np.sqrt(e.shape[1])), -1)\n",
    "        e = e[:, vert_token_start:vert_token_end, hor_token_start:hor_token_end]\n",
    "        all_embeddings[i] = e.reshape(e.shape[0], -1, e.shape[-1])\n",
    "    \n",
    "    print(all_embeddings[0].shape)\n",
    "\n",
    "    if aggregation_mode == \"mean\":\n",
    "        print(\"mean pool aggregation\")\n",
    "        all_mean_embeddings = [e.mean(axis=1) for e in all_embeddings]\n",
    "        all_embeds = np.concatenate(all_mean_embeddings, axis=0)\n",
    "    if aggregation_mode == \"max\":\n",
    "        print(\"max pool aggregation\")\n",
    "        all_sum_embeddings = [e.max(axis=1) for e in all_embeddings]\n",
    "        all_embeds = np.concatenate(all_sum_embeddings, axis=0)  \n",
    "\n",
    "    np.save(f\"{dir_name}{split}_{aggregation_mode}_only_data_{length}recordinglength.npy\", all_embeds)\n",
    "\n",
    "# Save raw recordings as well\n",
    "all_recordings = []\n",
    "for idx, batch in enumerate(tqdm(dataloader_batched)):\n",
    "    signal = batch[\"pixel_values\"]\n",
    "    recording = signal.flatten(start_dim = 1)\n",
    "    recording = np.array(recording, dtype=np.float32)\n",
    "    all_recordings.append(recording)\n",
    "all_recordings = np.vstack(all_recordings)\n",
    "all_recordings.shape\n",
    "\n",
    "np.save(f\"{dir_name}all_recordings_{length}length.npy\", all_recordings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533e4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting CLS Tokens and Raw Recordings With PCA\n",
    "aggregation_mode = \"cls\" # 'cls', 'mean', or 'max'\n",
    "extraction_mode = \"all\" \n",
    "load_path = f\"{dir_name}{split}_{aggregation_mode}_{extraction_mode}_200recordinglength.npy\"\n",
    "all_cls_tokens = np.load(load_path)\n",
    "all_cls_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "n_components = 200\n",
    "filename = f\"{dir_name}pca_obj_cls_tokens_{length}length_{n_components}components.pkl\"\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(all_cls_tokens)\n",
    "print(\"pca.n_components_:\", pca.n_components_)\n",
    "print(\"pca.n_features_in_:\", pca.n_features_in_)\n",
    "print(\"pca.components_.shape:\", pca.components_.shape)\n",
    "print(\"pca.explained_variance_.shape:\", pca.explained_variance_.shape)\n",
    "print(\"pca.explained_variance_ratio_.shape:\", pca.explained_variance_ratio_.shape)\n",
    "print(\"pca.singular_values_.shape:\", pca.singular_values_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_tokens_pca_reduced = pca.transform(all_cls_tokens)\n",
    "print(cls_tokens_pca_reduced.shape)\n",
    "np.save(f\"{dir_name}{split}_pca_reduced_cls_tokens_200components.npy\", cls_tokens_pca_reduced)\n",
    "with open(filename, 'wb') as pickle_file:\n",
    "        pickle.dump(pca, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = f\"{dir_name}{split}_{aggregation_mode}_{extraction_mode}_200recordinglength.npy\"\n",
    "all_embeds = np.load(load_path)\n",
    "all_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0472feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_ex = cls_tokens_pca_reduced.shape[0]\n",
    "embed_pca_components_list = [cls_tokens_pca_reduced[idx] for idx in range(total_num_ex)]\n",
    "test_ds = test_ds.add_column(name=\"embed_pca_components\", column=embed_pca_components_list)\n",
    "total_num_ex = all_embeds.shape[0]\n",
    "all_embeds_list = [all_embeds[idx] for idx in range(total_num_ex)]\n",
    "test_ds = test_ds.add_column(name=\"whole_embed\", column=all_embeds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13522904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_variable_normalization(labels_nonnan, variable_of_interest_col_name):\n",
    "    print(\"Normalizing variable:\", variable_of_interest_col_name)\n",
    "    if variable_of_interest_col_name in [\"PCL.Score\", \"GAD7.Severity\"]:\n",
    "        labels_nonnan_log1p = np.log(np.array(labels_nonnan, dtype=np.float32) + 1.0)  # log base_e(value + 1)\n",
    "        labels_nonnan_log1p_divmax = np.divide(labels_nonnan_log1p, labels_nonnan_log1p.max())  # bring into range [0, 1]\n",
    "        labels_normalized = labels_nonnan_log1p_divmax.tolist()\n",
    "    elif variable_of_interest_col_name == \"Age.At.MHQ\":\n",
    "        z_score_transform = StandardScaler()\n",
    "        labels_normalized_np = z_score_transform.fit_transform(np.expand_dims(np.array(labels_nonnan), axis=1))\n",
    "        labels_normalized_np = np.squeeze(labels_normalized_np, axis=1)\n",
    "        labels_normalized = labels_normalized_np.tolist()\n",
    "    elif variable_of_interest_col_name == \"Neuroticism\":\n",
    "        labels_nonnan_log1p = np.array(labels_nonnan, dtype=np.float32)  # if is Neuroticism, then no log transform, distribution is already good\n",
    "        labels_nonnan_log1p_divmax = np.divide(labels_nonnan_log1p, labels_nonnan_log1p.max())  # bring into range [0, 1]\n",
    "        labels_normalized = labels_nonnan_log1p_divmax.tolist()\n",
    "    elif variable_of_interest_col_name == \"PHQ9.Severity\":\n",
    "        labels_normalized = [1 if num > 4.0 else 0 for num in labels_nonnan]\n",
    "        sum_ones = sum(labels_normalized)\n",
    "        sum_zeros = len(labels_normalized) - sum_ones\n",
    "    elif variable_of_interest_col_name in [\"Depressed.At.Baseline\", \"Self.Harm.Ever\", \"Not.Worth.Living\"]:\n",
    "        labels_normalized = labels_nonnan\n",
    "        sum_ones = sum(labels_normalized)\n",
    "        sum_zeros = len(labels_normalized) - sum_ones\n",
    "    elif variable_of_interest_col_name in ['Gender', \"Task_Type\"]:\n",
    "        labels_normalized = labels_nonnan\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown variable of interest specified.\")\n",
    "    \n",
    "    return np.array(labels_normalized)\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def run_svm_regression(variable_of_interest, use_whole=False):\n",
    "    assert variable_of_interest in [\"Age.At.MHQ\", \"PHQ9.Severity\", \"Neuroticism\", \"PCL.Score\", \"GAD7.Severity\"], \\\n",
    "        \"Please specify a metadata variable with a range of continuous values.\"\n",
    "\n",
    "    # Define custom scoring metrics\n",
    "    scoring = [\"r2\",\"neg_mean_squared_error\"]\n",
    "    \n",
    "    # Select non-nan samples for metadata variable\n",
    "    full_label_list = test_ds[variable_of_interest]\n",
    "    non_nan_indices = [idx for idx in range(len(full_label_list)) if not math.isnan(full_label_list[idx])]\n",
    "\n",
    "    non_nan_ds = test_ds.select(non_nan_indices)\n",
    "\n",
    "    # Shuffle dataset reproducibly\n",
    "    non_nan_ds = non_nan_ds.shuffle(seed=42)\n",
    "\n",
    "    # Get PCA components for raw data and CLS tokens after shuffling\n",
    "    if use_whole:\n",
    "        embed_pca_nonnan = np.array(non_nan_ds[\"whole_embed\"], dtype=np.float32)\n",
    "    else:\n",
    "        embed_pca_nonnan = np.array(non_nan_ds[\"embed_pca_components\"], dtype=np.float32)\n",
    "\n",
    "\n",
    "    # Get labels\n",
    "    labels = non_nan_ds[variable_of_interest]\n",
    "    labels = [int(num) for num in labels]\n",
    "\n",
    "    # Normalize target variable for regression\n",
    "    labels_normalized_np = target_variable_normalization(\n",
    "        labels_nonnan=labels, \n",
    "        variable_of_interest_col_name=variable_of_interest\n",
    "    )\n",
    "    labels_normalized = labels_normalized_np.tolist()\n",
    "    print(\"Max and min:\", labels_normalized_np.max(), labels_normalized_np.min())\n",
    "    \n",
    "    \n",
    "    \n",
    "    #--- Fit MLP on CLS Token PCA Components ---#\n",
    "    regr = svm.LinearSVR()\n",
    "#     scores = cross_val_score(regr, cls_token_pca_nonnan, labels_normalized, cv=5, \n",
    "#                              scoring=\"neg_mean_squared_error\")\n",
    "    results_dict = cross_validate(regr, embed_pca_nonnan, labels_normalized, cv=5, \n",
    "                             scoring=scoring)\n",
    "    r2_scores = results_dict[\"test_r2\"]\n",
    "    mse_scores = results_dict[\"test_neg_mean_squared_error\"]\n",
    "    mse_scores = [-1 * num for num in mse_scores]\n",
    "    print(f\"CLS Token PCA Component MSE: {statistics.mean(mse_scores):.3f} +/- {statistics.stdev(mse_scores):.3f}\")\n",
    "    print(f\"CLS Token PCA Component R2: {statistics.mean(r2_scores):.3f} +/- {statistics.stdev(r2_scores):.3f}\")\n",
    "\n",
    "def run_svm_classification(variable_of_interest, use_whole=False):\n",
    "    assert variable_of_interest in [\"Gender\",\"PHQ9.Severity\",\"Task_Type\"], \\\n",
    "        \"Please specify a metadata variable with a binary value.\"\n",
    "\n",
    "    # Define custom scoring metrics\n",
    "    scoring = [\"accuracy\",\"balanced_accuracy\",\"roc_auc\",\"f1\"]\n",
    "    \n",
    "    # Select non-nan samples for metadata variable\n",
    "    full_label_list = test_ds[variable_of_interest]\n",
    "    non_nan_indices = [idx for idx in range(len(full_label_list)) if not math.isnan(full_label_list[idx])]\n",
    "\n",
    "    non_nan_ds = test_ds.select(non_nan_indices)\n",
    "\n",
    "    # Shuffle dataset reproducibly\n",
    "    non_nan_ds = non_nan_ds.shuffle(seed=42)\n",
    "\n",
    "    # Get PCA components for raw data and CLS tokens after shuffling\n",
    "    if use_whole:\n",
    "        embed_pca_nonnan = np.array(non_nan_ds[\"whole_embed\"], dtype=np.float32)\n",
    "    else:\n",
    "        embed_pca_nonnan = np.array(non_nan_ds[\"embed_pca_components\"], dtype=np.float32)\n",
    "\n",
    "\n",
    "    # Get labels\n",
    "    labels = non_nan_ds[variable_of_interest]\n",
    "    labels = [int(num) for num in labels]\n",
    "\n",
    "    # Normalize target variable for regression\n",
    "    labels_normalized_np = target_variable_normalization(\n",
    "        labels_nonnan=labels, \n",
    "        variable_of_interest_col_name=variable_of_interest\n",
    "    )\n",
    "    labels_normalized = labels_normalized_np.tolist()\n",
    "    print(\"Max and min:\", labels_normalized_np.max(), labels_normalized_np.min())\n",
    "    \n",
    "    \n",
    "    \n",
    "    #--- Fit MLP on CLS Token PCA Components ---#\n",
    "    regr = svm.SVC()\n",
    "#     scores = cross_val_score(regr, cls_token_pca_nonnan, labels_normalized, cv=5, \n",
    "#                              scoring=\"neg_mean_squared_error\")\n",
    "    results_dict = cross_validate(regr, embed_pca_nonnan, labels_normalized, cv=5, \n",
    "                             scoring=scoring)\n",
    "    # print('results_dict: ',results_dict)\n",
    "    accuracy_score = results_dict[\"test_accuracy\"]\n",
    "    balanced_accuracy_score = results_dict[\"test_balanced_accuracy\"]\n",
    "    roc_auc_score = results_dict[\"test_roc_auc\"]\n",
    "    f1_score = results_dict[\"test_f1\"]\n",
    "    if use_whole:\n",
    "        print(f\"CLS Token whole data accuracy: {statistics.mean(accuracy_score):.3f} +/- {statistics.stdev(accuracy_score):.3f}\")\n",
    "        print(f\"CLS Token whole data balanced_accuracy: {statistics.mean(balanced_accuracy_score):.3f} +/- {statistics.stdev(balanced_accuracy_score):.3f}\")\n",
    "        print(f\"CLS Token whole data roc_auc: {statistics.mean(roc_auc_score):.3f} +/- {statistics.stdev(roc_auc_score):.3f}\")\n",
    "        print(f\"CLS Token whole data f1: {statistics.mean(f1_score):.3f} +/- {statistics.stdev(f1_score):.3f}\")\n",
    "    else: \n",
    "        print(f\"CLS Token PCA data accuracy: {statistics.mean(accuracy_score):.3f} +/- {statistics.stdev(accuracy_score):.3f}\")\n",
    "        print(f\"CLS Token PCA data balanced_accuracy: {statistics.mean(balanced_accuracy_score):.3f} +/- {statistics.stdev(balanced_accuracy_score):.3f}\")\n",
    "        print(f\"CLS Token PCA data roc_auc: {statistics.mean(roc_auc_score):.3f} +/- {statistics.stdev(roc_auc_score):.3f}\")\n",
    "        print(f\"CLS Token PCA data f1: {statistics.mean(f1_score):.3f} +/- {statistics.stdev(f1_score):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30994a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_variable = \"Age.At.MHQ\"\n",
    "age_results = run_svm_regression(metadata_variable, use_whole=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_variable = \"GAD7.Severity\"\n",
    "gad7_results = run_svm_regression(metadata_variable, use_whole=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692685a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_variable = \"Neuroticism\"\n",
    "phq9_results = run_svm_regression(metadata_variable, use_whole=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a177ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_variable = \"PCL.Score\"\n",
    "pcl_results = run_svm_regression(metadata_variable, use_whole=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b325c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_variable = \"PHQ9.Severity\"\n",
    "phq_results = run_svm_classification(metadata_variable, use_whole=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f28fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_variable = \"Gender\"\n",
    "gender_results = run_svm_classification(metadata_variable, use_whole=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554c88ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_variable = \"Task_Type\"\n",
    "task_results = run_svm_classification(metadata_variable, use_whole=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64cdecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
